\chapter{Concentration Inequalities and the Laws of Large Numbers}

\section{Introduction}
How do we estimate the bias of a binary random experiment like coin-flipping? How do we measure the overall deviation of an expected hypothetical result and what was really observed? How do we know if the coin is a cheated coin?

Such a quantity for a random experiment of $n$ trials can be expressed as $\hat{p} = \frac{1}{n} S_n$, where $S_n$ is the number of success in a binary experiment. \\
And, let $p$ denote the true bias of the coin, which is unknown, and we are offered that $\E[S_n] = np$, we see that the estimator $\hat{p}$ works to align with $p$.

Therefore,
\begin{quote}
    This means when $n$ is sufficiently large, the estimator of bias is close to the true value of bias.
\end{quote}
We will discuss this rule again later.

But, how large should $n$ be, so to provide an upper bound to the error between the estimator and bias itself? \\
We do not truely know, because the estimator of bias itself is also a random variable. \\
Let us analyze the estimator in the perspectives of random variable:
\[\hat{p} = \frac{1}{n} S_n = \frac{1}{n} \sum_{i = 1}^n X_n\]
where $X_i$ is an indicator random variable that suggests whether the $i^{th}$ trial of experiment was noted as successful. This means the estimator $\hat{p}$ is an average of indicator random variables, therefore also a random variable itself.

It is somehow reasonable that there is no concrete theorem for an upperbound of $n$ such that the estimator estimates the real bias of some binary experiment at extreme accuracy. Or else I wouldn't be malding over gacha games and hack with mathematics. \\
But, rather than trying to guarantee that the error of estimation has some upperbound, we can demand it to have an upper bound with some confidence $1 - \delta$, therefore:
\[\P[|\hat{p} - p| \leq \epsilon] \geq 1 - \delta\]
Spoiler alert: to guarantee and so, we require that:
\[n \geq \frac{1}{4 \epsilon^2 \delta}\]
We will also discuss this later within the section of \textit{Chebyshev's Inequality}.

\section{Markov's Inequality}
Let us discuss the upper bound of error for experiments on nonnegative random variables:
\begin{ln-theorem}{Markov's Inequality}{}
    For a nonnegative random variable $X$ with finite mean, for any positive constant $c$:
    \[
        \P[X \geq c] \leq \frac{\E[X]}{c}
    \]
    \tcblower
    \textbf{Proof}: \\
    Let $\mathcal{A}$ denote the range of values that random variable $X$ is in. Then:
    \begin{align*}
        \E[X]
        &= \sum_{a \in \mathcal{A}} a \times \P[X = a] \\
        &\geq \sum_{a \geq c} a \times \P[X = a] \\
        &\geq \sum_{a \geq c} c \times \P[X = a] \\
        &= c \sum_{a \geq c} \P[X = a] \\
        &= c \P[X \geq c]
    \end{align*}
\end{ln-theorem}
Here, say we let $c = 2 \E[X]$, then we see that at most half of the individuals can reach some score that is double the mean in a game. \\

We can further generalize Markov's Inequality onto random variables based on absolute values:
\begin{ln-theorem}{Generalized Markov's Inequality}{}
    Let $Y$ be an arbitrary random variable with finite mean. Then, for any positive constants $c$, $r$,
    \[
        \P[|Y| \geq c] \leq \frac{\E[{|Y|}^r]}{c^r}
    \]
    \tcblower
    \textbf{Proof}: \\
    For $c > 0$, $r > 0$, let $I$ be an indicator function:
    \[
        {|Y|}^r \geq {|Y|}^r I{(|Y| \geq c)} \geq c^r I{(|Y| \geq c)}
    \]
    Taking expectations of both sides then offers that:
    \[
        \E[{|Y|}^r] \geq c^r \E[I{(|Y| \geq c)}] = c^r \P[I{(|Y| \geq c)}]
    \]
\end{ln-theorem}
Markov's inequality also concerns the center of its probability distribution: $\mu = \E[X]$, as the fulcrum of distributional mass. \\
To let the mass on left of and right of $\mu$ be equivalent, up until some point at the right $k \mu$, the mass must be $0$, while to keep balance, the mass at the left must than be $(k - 1)$ times of the mass at the right:
\[(k - 1) {mass}_{right} \times 1 = {mass}_{right} \times (k - 1)\]
This forms a known phenomenon called ``Markov's Bound'' with $\alpha = k \mu$, where $\P[X > k\mu] = 0$ can be noted as some constraint of a distribution with $k$ being a value as regulated by the fulcrum-determined balance addressed above.

\begin{ln-quest}{Coin Tosses and Markov}{}
    Consider fair coin toss, and let $X$ denote the number of heads observed. \\
    What is the probability of observing more than $\frac{3}{4} n$ heads? Let's apply Markov inequality to obtain an upper bound on $\P[X \geq \frac{3}{4} n]$, and since $\E[X] = \frac{1}{2} n$:
    \[
        \P[X \geq \frac{3}{4} n] \leq \frac{\E[X]}{c} = \frac{2}{3}
    \]
\end{ln-quest}

\section{Chebyshev's Inequality}
\begin{ln-theorem}{Chebyshev's Inequality}{}
    For a random variable $X$ with finite expectation $\E[X] = \mu$, for any positive constant $c$:
    \[
        \P[|X - \mu| \geq c] \leq \frac{Var(X)}{c^2}
    \]
    \tcblower
    \textbf{Proof}:
    Let $Y = {(X - \mu)}^2$, then we discover that $\E[Y] = Var(X)$. \\
    Meanwhile, this presents us that we are looking for the probability where $Y \geq c^2$, which we may apply Markov's Inequality from, to derive the following result:
    \[
        \P[|X - \mu| \geq c] = \P[Y \geq c^2] \leq \frac{\E[Y]}{c^2} = \frac{Var(X)}{c^2}
    \]
\end{ln-theorem}
In fact, since variance is $\sigma^2$, we may understand the Chebyshve's inequality as the following Corollary:
\begin{quote}
    For any random variable $X$ with finite expectation $\mu$ and finite standard deviation $\sigma$:
    \[\P[|X - \mu| \geq k \sigma] \leq \frac{\sigma^2}{k^2 \sigma^2} = \frac{1}{k^2}\]
\end{quote}

\section{The Law of Large Numbers}
\begin{ln-theorem}{Law of Large Numbers}{}
    Let $X_1, \cdots, X_n$ be a sequence of Independent and Identically Distributed random variables with common finite expectation $\E[X_i] = \mu$ for all $i$, then their partial sum: $S_n = \sum_{i = 1}^n X_i$:
    \[
        \lim_{n \rightarrow \infty} \P \bigg[ \bigg|\frac{1}{n} S_n - \mu \bigg| < \epsilon \bigg] = 1
    \]
    for any positive constant $\epsilon$.
    \tcblower
    \textbf{Proof}:
    Let $Var(X_i) = \sigma_2$ be the common variance of random variables, we assume that $\sigma^2$ is finite. \\
    Then, we can obtain from known means and variances that:
    \[
        \E \bigg[\frac{1}{n} S_n \bigg] = \mu
    \]
    \[
        Var \bigg(\frac{1}{n} S_n \bigg) = \frac{1}{n^2} Var(S_n) = \frac{1}{n^2} \sum_{i = 1}^n Var(X_i) = \frac{\sigma^2}{n}
    \]
    Therefore, by Chebyshev's inequality, we have:
    \[
        \P \bigg[ \bigg|\frac{1}{n} S_n - \mu \bigg| \geq \epsilon \leq \frac{\frac{\sigma^2}{n}}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \bigg]
    \]
    Which we can see that the limit of such value approaches $0$ when $n$ approaches $\infty$. \\
    Therefore, its complement would approach $0$ when $n$ approaches $\infty$.
\end{ln-theorem}
