\chapter{Conditional Probability, Independence, Combination of Events}
Chapter Description.

\section{Conditional Probability}
Sometimes, the sample space available to us at the moment depends on the events that has occurred before it. \\
For example, if we have sampled without replacement, the probability of getting a particular sample point would change due to the change in sample space. \\
Mathematicians have organized a uniform way to describe such phenomenon where the probability of an event changes based on the event that transpires before it:
\begin{ln-define}{Conditional Probability}{}
    Let $A$ denote the event which we'd like to know the probability of. \\
    Then, let $B$ be the event that has already happened and influences the probability of event $A$: for example, some urn is empty provided a ball-and-urn situation, or a ball of some color has been priorly sampled without replacement from a finite pool of balls. \\
    The \textbf{conditional probability} of $A$ given $B$ is then the probability of event $A$ provided that event $B$ has occurred, denoted mathematically as:
    \[\P[A|B]\]
\end{ln-define}
The computation of such probability can be portrayed with a venn diagram. \\
If event $B$ has occurred, then the sample points we count towards our conditional probability would only concern sample points that signify event $B$. Among which, the sample points that signify both event $A$ and $B$ is in the sample space $A \cap B$. \\
Therefore:
\begin{ln-define}{Computation of Conditional Probability}{}
    For events $A, B \subset \Omega$ in the same probability space such that $\P[B] > 0$, the conditional probability of $A$ given $B$ is
    \[\P[A|B] = \frac{\P[A \cap B]}{\P[B]}\]
\end{ln-define}
Now, if you wouldn't mind an example to demonstrate the above:
\begin{ln-quest}{Conditional Probability of Card Dealing Instance}{}
    What is the conditional probability that, when dealing 2 cards from a normal deck of poker cards, given the first card is an ace, the second card is also an ace?
    \tcblower
    In this case, $\P[B]$ is the probability of the first card being an ace:
    \[\frac{4}{52}\]
    The probability of $\P[A \cap B]$ is the probability of both cards being an ace:
    \[\frac{4}{52} \frac{3}{51}\]
    Therefore:
    \[\P[A|B] = \frac{\P[A \cap B]}{\P[B]} = \frac{3}{51}\]
\end{ln-quest}
Let us do a sanity check for the above example. If we are provided that one ace is already drawn, then it should become harder for the next drawn card to also be ace. \\
This intuition is coherent with the results of our computation: the original probability of drawing an ace is $\frac{4}{52}$, which is higher than the conditional probability we computed.

\section{Bayesian Inferenece}
Conditional probability offers us the opportunity to explore a probability provided some update to the current state of system. For example, in the above card dealing example, by computing the conditional probability of a situation, we computed the probability of something happening provided an update in the state (contents of deck) of the system (deck). \\
Such mathematical device lends hand to an important mathematical subject, called \textbf{Bayesian Inference}: regarding how to update knowledge after making an observation. In such interpretation, we can treat $\P[A]$ in the computation of $\P[A|B]$ as a ``priori'', and the conditional probability itself a ``posterior''. \\
Let us inspect the following famous application of conditional probability to get a better idea of Bayesian Inference:
\begin{ln-quest}{Conditional Probability: Accuracy of Clinical Tests}{}
    Provided the following results of a clinical trial:
    \begin{itemize}
        \item When applied to an affected person, the test is positive in $90\%$ of cases, and negative in $10\%$ of cases.
        \item When applied to a negative person, the test is positive in $20\%$ of cases, and negative in $80\%$ of cases.
    \end{itemize}
    Suppose $5\%$ of the US population is truly affected (which is our priori), what is the probability that a random person in the US Population is affected, provided the person tests positive?
    \tcblower
    First of all, the answer is NOT just $5\%$. Those who are affected can also errorneously be tested as negative. \\
    To consider this question in its entire scope, we can construct the following table:
    \begin{center}
        \begin{tabular}{c||c|c}
            & Affected & Healthy \\
            \hline
            \hline
            Positive & AP & HP \\
            \hline
            Negative & AN & HN
        \end{tabular}
    \end{center}
    (Note that we would usually say AP is True Positive, HN is True Negative, HP is False Positive... etc.) \\
    In this case, the probability that a tested person is affected given the person tests positive is in fact the conditional probability:
    \[\P[\text{AP}|\text{Positive}] = \frac{\P[AP]}{\P[AP] + \P[HP]}\]
    Now, using the priori that \textit{$5\%$ of the US population is truly affected}, let's fill in the table above with concrete probability values:
    \begin{center}
        \begin{tabular}{c||c|c}
            & Affected & Healthy \\
            \hline
            \hline
            Positive & $0.05 \times 0.9$ & $0.95 \times 0.2$ \\
            \hline
            Negative & $0.05 \times 0.1$ & $0.95 \times 0.8$
        \end{tabular}
    \end{center}
    Substituting the value, we calculate the conditional probability to be:
    \[\P[\text{AP}|\text{Positive}] = \frac{\P[AP]}{\P[AP] + \P[HP]} = \frac{0.05 \times 0.9}{0.05 \times 0.9 + 0.95 \times 0.8} ~ 0.1914\]
\end{ln-quest}
Let's explore another method of performing the above computation.
\begin{ln-quest}{An Alternative Solution}{}
    Note that for events $A$ and $B$, there are two ways to express the probability $\P[A \cap B]$:
    \[\P[A \cap B] = \P[A|B] \P[B] = \P[B|A] \P[A]\]
    \[\P[B|A] = \frac{\P[A \cap B]}{\P[A]}\]
    Combining the above with the conditional probability formula, we learned that:
    \[\P[A|B] = \frac{\P[A \cap B]}{\P[B]} = \frac{\P[B|A] \P[A]}{\P[B]}\]
    Since an event $A$ either occurs or not, we can also conclude that:
    \begin{align*}
        \P[B] = \P[A \cap B] + \P[\overline{A} \cap B]
        &= \P[B|A] \P[A] + \P[B|\overline{A}] \P[\overline{A}] \\
        &= \P[B|A] \P[A] + \P[B|\overline{A}] (1 - \P[A])
    \end{align*}
    Combining all above equations, we obtain that:
    \[
        \P[A|B] = \frac{\P[A \cap B]}{\P[B]} = \frac{\P[A \cap B]}{\P[B|A] \P[A] + \P[B|\overline{A}] (1 - \P[A])}
    \]
    Substituting event $A$ with being an affected individual and event $B$ with the probability of testing positive, we would eventually reach the same computational result as outlined before!
\end{ln-quest}

\section{Bayes Rule and Total Probability Rule}
Turns out our prior efforts have already been found by prior mathematicians, who then named them as follows:
\begin{ln-define}[sidebyside]{Bayes Rule and Total Probability Rule}{}
    \textbf{Bayes Rule}:
    \[\P[B|A] = \frac{\P[A \cap B]}{\P[A]}\]
    \tcblower
    \textbf{Total Probability Rule}:
    \[\P[B] = \P[B|A] \P[A] + \P[B|\overline{A}] (1 - \P[A])\]
\end{ln-define}
Let's demonstrate the above principles with some examples:

\subsection{Example 1: Tennis Match}
The current player is now fighting for the place of top 3 in a tournament, and there are two possible opponents for the current player: $X$ and $Y$. \\
If person $X$ is the opponent, then the current player has a $70\%$ chance of winning; otherwise, if the opponent is $Y$, the chance of winning becomes $30\%$. \\
The chance of getting $X$ as the opponent is $60\%$. \\
Provided the above, what is the probability that the current player wins?
\begin{ln-quest}{Example 1: Tennis Match}{}
    Let us first label each possibilities we compute the probability for as an event:
    \begin{itemize}
        \item Let event $A$ be that the current player wins.
        \item Let event $B_X$ be that $X$ is the opponent.
        \item Let event $B_Y$ be that $Y$ is the opponent.
    \end{itemize}
    We now calculate $\P[A]$, and by the Total Probability Rule, we may provide that:
    \begin{align*}
        \P[A] &= \P[A|B_X] \P[B_X] + \P[A|B_Y] \P[B_Y] \\
        &= 0.7 \times 0.6 + 0.3 \times 0.4 = 0.54
    \end{align*}
\end{ln-quest}

\subsection{Example 2: Balls and Bins}
Suppose we are provided two bins, each filled with different contents of balls:
\begin{itemize}
    \item The first bin, $B_1$, has two white, three black balls.
    \item The second bin, $B_2$, has one white, one black ball.
\end{itemize}
We choose a random bin to sample a ball from, both bins having the same probability of being picked. Then, what is the probability that we picked $B_1$ provided that a white ball was drawn?
\begin{ln-quest}{Example 2: Balls and Bins}{}
    Let us, once again, first label each assessing possibilities as events:
    \begin{itemize}
        \item Let event $A$ be that $B_1$ was chosen.
    \end{itemize}
    Based on observation, we may also have concluded that:
    \begin{itemize}
        \item The probability of drawing a white ball and having picked $B_1$ was $\P[w \cap A] = \frac{2}{5} \times \frac{2}{1} = \frac{1}{5}$.
        \item The probability of having picked $B_1$ was $\P[A] = \frac{1}{2}$.
        \item The probability of having drawed a white ball was $\P[w] = \P[A] \times \frac{2}{5} + \P[\overline{A}] \times \frac{1}{2} = \frac{9}{20}$.
    \end{itemize}
    Using Bayes' Rule:
    \begin{align*}
        \P[A|w] &= \frac{\P[A \cap w]}{\P[w]} \\
        &= \frac{\frac{2}{10}}{\frac{9}{20}} = \frac{4}{9}
    \end{align*}
\end{ln-quest}

\subsection{Generalization of Bayes Rule and Total Probability Rule}
To consider these principles of probability in a more general concept across an event being divisble into two aspects, let us first discuss how events can be partitioned. \\
In other words, rather than only considering that an event only has two cases (positive or negative, head or tails), let us discuss how an event can be divided into having $n \geq 2$ cases (for example, picking from a pool of balls in $n$ colors):
\begin{ln-define}{Partition of Event}{}
    An event $A$ is partitioned into $n$ (sub)events $A_1, \cdots, A_n$ if:
    \begin{bindenum}
        \item $A = A_1 \cup \cdots \cup A_n$
        \item {
            All such subevents (partitions) are pairwise disjoint. That is,
            \[\forall (i, j) \in \{(x, y) : x \neq y \land 1 \leq x \leq y \leq n\}, A_i \cap A_j = \emptyset\]
        }
    \end{bindenum}
    Therefore, each outcome in $A$ belongs to exactly one of the subevents $A_1, \cdots, A_n$.
\end{ln-define}
Upon this view of partitions, we may generalize as such:
\begin{ln-define}{Generalization of Bayes Rule and Total Probability Rule}{}
    Let $A_1, \cdots, A_n$ be a partition of sample space $\Omega$. \\
    The Total Probability Rule of event $B$ is:
    \[\P[B] = \sum_{i = 1}^n \P[B \cap A_i] = \sum_{i = 1}^n \P[B | A_i] \P[A_i]\]
    The Bayes' Rule of event $A_i$ given $B$ is:
    \[\P[A_i | B] = \frac{\P[B | A_i] \P[A_i]}{\P[B]} = \frac{\P[B | A_i] \P[A_i]}{\sum_{j = 1}^n \P[B | A_j] \P[A_j]}\]
\end{ln-define}

\section{Combination of Events}
In Computer Science, we frequently discuss the probability of some combination of events. This has sizeable implication on the operations of Data Science as well as many other significant contributions and subfields of computer science. \\
Therefore, it is worthy to discuss the probability theories of combination of events with this section. We will first introduce independent events, then the intersections of events.

\subsection{Independent Events}
Let us first define the term ``independent events'':
\begin{ln-define}{Independent Event}{}
    Two events $A$, $B$ in the same probability space are independent if $\P[A \cap B] = \P[A] \times \P[B]$.
\end{ln-define}
Independence, verbosely, would then mean that the probability of event $A$ is not influenced by whether or not $B$ occurs. Therefore, under such context, $\P[A|B] = \P[A]$. \\
Let us discuss independent events on some wider scale:
\begin{ln-define}{Mutual Independence}{}
    Events $A_1, \cdots, A_n$ are said to be mutually independent if for every subset $I \subseteq \{1, \cdots, n\}$ with cardinality greater than $1$:
    \[\P[\cap_{i \in I} A_i] = \prod_{i \in I} \P[A_i]\]
    Equivalently, we can express such expression in an alternative form: for all $B_i \in \{A_i, \overline{A_i}\}$:
    \[\P[B_1 \cap \cdots \cap B_n] = \prod_{i = 1}^n \P[B_i]\]
    Using a pair-by-pair perspective, we are also able to find that, for mutually independent events $A_1, \cdots, A_n$:
    \[\P[A_i | \cap_{j \in I} A_j] = \P[A_i]\]
\end{ln-define}
However, it is important to note that pair-wise independence does not imply nutual independence, since an event can have dependence on a combination of event rather than a singular one.

\subsection{Intersection of Events}
Let us first consider what we have learned from bonditional probability:
\begin{ln-theorem}{Product Rule (Chain Rule)}{}
    For any events $A, B$, we may conclude:
    \[\P[A \cap B] = \P[A] \P[B | A] = \P[B] \P[A | B]\]
    On a general scale, for events $A_1, \cdots, A_n$:
    \[\P[\cap_{i = 1}^n A_i] = \P[A_1] \times \P[A_2 | A_1] \times \P[A_3 | A_2 \cap A_1] \times \cdots \times \P[A_n | \cap_{i = 1}^{n-1} A_i]\]
    \tcblower
    \textbf{Reasoning}: Let us perform a mathematical induction on $n$. \\
    At base case, we have one event to consider, and the trivial case holds. \\
    At an inductive case, for an arbitrary $n > 1$, let the induction hypothesis be that:
    \[\P[\cap_{i = 1}^{n - 1} A_i] = \P[A_1] \times \P[A_2 | A_1] \times \P[A_3 | A_2 \cap A_1] \times \cdots \times \P[A_{n - 1} | \cap_{i = 1}^{n - 2} A_i]\]
    We may then apply the definition of conditional probability, where:
    \[\P[\cap_{i = 1}^{n - 1} A_i \cap A_n] = \P[\cap_{i = 1}^{n - 1} A_i] \P[A_n | \cap_{i = 1}^{n - 1} A_i]\]
    Which leads to the exact formula outlined by the Product Rule.
\end{ln-theorem}

\subsection{Union of Events}
The tricky part about calculating for the union of events is that: if we directly sum up the probability of members of unions, we make the calculation extremely frequently prone to overcount, if not always. \\
However, we have learned a principle that helps us to eliminate overcountings when counting over unions of sets: Inclusion-Exclusion.
\begin{ln-define}{Inclusion-Exclusion Principle in Probability}{}
    For events $A_1, \cdots, A_n$ in some probability space, where $n \geq 2$:
    \[\P[A_1 \cup \cdots \cup A_n] = \sum_{k = 1}^n ({(-1)}^{k - 1} \sum_{S \subseteq \{1, \cdots, n\}, |S| = K} \P[\cap_{i \in S} A_i])\]
    We see that this reflects to the original inclusion-exclusion principle:
    \[|A_1 \cup \cdots \cup A_n| = \sum_{k = 1}^n {(-1)}^{k - 1} \sum_{S \subseteq \{1, \cdots, n\}:|S| = k} |\cap_{i \in S}A_i|\]
\end{ln-define}

But when $n$ is large, ain't nobody got time for inclusion-exclusion! \\
So here are a bit of tips to deal with union of events when there are too many cases to consider:
\begin{enumerate}
    \item If events $A_1, \cdots, A_n$ that we consider are naturally Mutually Exclusive Events, then \[\P[A_1 \cup \cdots \cup A_n] = \sum_{i = 1}^n \P[A_i]\]
    \item Following the above point, we recognize an opportunity for sanity check in any calcualtion for unions of events:  \[\P[A_1 \cup \cdots \cup A_n] \leq \sum_{i = 1}^n \P[A_i]\]
\end{enumerate}
