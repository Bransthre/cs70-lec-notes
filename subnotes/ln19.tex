\chapter{Geometric and Poisson Distributions}

\section{Geometric Distribution}
The geometric distribution frequently occurs in situations where we consider ``how long we have to wait'' for something to occur, such as ``how many run until error'', or ``how many rolls until SSR''.

The \textbf{geometric distribution} is then mathematically defined as below:
\begin{ln-define}{Geometric Distribution}{}
    A random variable $X$ such that:
    \[\P[X = i] = {(1 - p)}^{i - 1} p, i \in \Z^+\]
    is said to have a geometric distribution with parameter $p$, mathematically expressable as:
    \[X \sim \text{Geometric}(p)\]
\end{ln-define}
This composition of the distribution seems reasonable! \\
Say, the probability at which you get an ultra rare item from a game is 0.8\%. Then, the probability at which you get this item at the $k^{th}$ attempt (ecah attempt is one attempt at the $0.8\%$ probability, as implied), is:
\[\P[X = k] = (1 - 0.008)^{k - 1} 0.008\]
as the geometric distribution implies.

All proabbilities calculated here are products of powers of nonnegative numbers; thus, will never produce negative probabilities. In that case, we only need the one-total property to hold true for this distribution to appear mathematically sound. \\
Does such distribution sum to $1$? Allow us to verify:
\begin{align*}
    \sum_{i = 1}^{\infty} \P[X = i]
    &= \sum_{i = 1}^{\infty} {(1 - p)}^{i - 1} p \\
    &= p \sum_{i = 1}^{\infty} {(1 - p)}^{i - 1} \\
    &= p \lim_{n \rightarrow \infty} \frac{1 - {(1 - p)}^{n}}{1 - (1 - p)} \\
    &= p \frac{1}{1 - 1 + p} = 1
\end{align*}

The distribution of $X$ would thus be some curve that decreases by a factor of $(1 - p)$ at each step; in other words, an exponential decay. We will see that it is, in fact, an analog to what we will later introduce as the exponential distribution, in the last chapters of these notes. \\
And from this above fact, we would know that the amplitude of a geometric distribution's curve is dependent on $p$, while the speed at which the curve's height descend across $x$ depends on $(1 - p)$.

\subsection{Mean and Variance of a Geometric RV}
Let us first discuss the expectation of a geometric random variable. \\
If directly computed, we would obtain some result like what follows:
\[\E[X] = \sum_{i = 1}^\infty i\P[X = i] = p\sum_{i = 1}^\infty i{(1 - p)}^{i - 1}\]
However, there is a way to simplify this above complication:
\begin{ln-theorem}{Tail Sum Formula}{}
    \textbf{Theorem}:
    \[\E[X] = \sum_{i = 1}^\infty \P[X \geq i]\]
    \tcblower
    \textbf{Proof}:
    \begin{align*}
        \E[X] &= \sum_{i = 1}^\infty i\P[X = i] \\
        &= p\sum_{i = 1}^\infty i{(1 - p)}^{i - 1} \\
        &= 0 p_0 + 1 p_1 + 2 p_2 + \cdots \\
        &= (p_1 + p_2 + \cdots) + (p_2 + \cdots) + \cdots \\
        &= \P[X \geq 1] + \P[X \geq 2] + \cdots
    \end{align*}
\end{ln-theorem}
What is the significance of this?
\begin{ln-theorem}{The Expectation of a Geometric RV}{}
    \begin{align*}
        \P[X \geq i] &= (1 - p){(1 - p)}^{i - 1} + p{(1 - p)}^{i - 1} \\
        &= {(1 - p)}^{i - 1} \\
        \E[X] &= \sum_{i = 1}^\infty \P[X \geq i] \\
        &= \sum_{i = 1}^\infty {(1 - p)}^{i - 1}
        = \frac{1}{1 - (1 - p)} = \frac{1}{p}
    \end{align*}
\end{ln-theorem}
Upon that, let's also discuss and derive such identity:
\begin{ln-theorem}{Expectation of X(X - 1)}{}
    \textbf{Theorem}:
    \[\E[X(X - 1)] = \frac{2(1 - p)}{p^2}\]
    \tcblower
    \textbf{Proof}:
    Preliminarily,
    \begin{align*}
        \E[X(X - 1)]
        &= \sum_{i = 1}^\infty i(i - 1) \times \P[X = i] \\
        &= \sum_{i = 1}^\infty i(i - 1) \times p{(1 - p)}^{i - 1} \\
        &= p(1 - p) \sum_{i = 1}^\infty i(i - 1) {(1 - p)}^{i - 2}
    \end{align*}
    With some calculus and an identity we just learned,
    \begin{align*}
        \sum_{i = 1}^\infty {(1 - p)}^i &= \frac{1}{p} \\
        \frac{d}{dp} \bigg( \frac{1}{p} \bigg) &= \sum_{i = 1}^\infty -i{(1 - p)}^{i - 1} \\
        &= -\frac{1}{p^2} \\
        \frac{d^2}{di^2} \bigg( \frac{1}{p} \bigg) &= \sum_{i = 1}^\infty i(i - 1) {(1 - p)}^{i - 2} \\
        &= \frac{2}{p^3}
    \end{align*}
    Using the result above:
    \begin{align*}
        \E[X(X - 1)]
        &= p(1 - p) \sum_{i = 1}^\infty i(i - 1) {(1 - p)}^{i - 2} \\
        &= p(1 - p) \frac{2}{p^3} \\
        &= \frac{2(1 - p)}{p^2}
    \end{align*}
\end{ln-theorem}

Using the linearity of expectation, we can elegantly calculate the variance of a geometric distribution:
\begin{ln-theorem}{The Variance of a Geometric RV}{}
    \textbf{Theorem}:
    \[Var(X) = \frac{1 - p}{p^2}\]
    \tcblower
    \textbf{Proof}:
    \begin{align*}
        Var(X) &= \E[X^2] - {(\E[X])}^2 \\
        &= \E[X^2 - X] + \E[X] - {(\E[X])}^2 \\
        &= \frac{2(1 - p)}{p^2} + \frac{1}{p} - \frac{1}{p^2} \\
        &= \frac{2 - 2p + p - 1}{p^2} = \frac{1 - p}{p^2}
    \end{align*}
\end{ln-theorem}

In addition, geometric distributions have a property called ``memoryless'':
\begin{ln-define}{Memoryless}{}
    In memoryless distributions, prior trials do not affect the number of additional trials needed. Mathematically:
    \[\P[X > n + m | X > m] = \P[X > n]\]
    For example, say we have already tossed the coin $m$ times, what is the probability that we need more than $n$ additional tosses before getting our first head?
    \begin{align*}
        \P[X > n + m | X > m]
        &= \frac{\P[X > n + m]}{\P[X > m]} \\
        &= \frac{{(1 - p)}^{n + m}}{{(1 - p)}^m} \\
        &= {(1 - p)}^n = \P[X > n]
    \end{align*}
\end{ln-define}
In this sense, ``memorylessness'' refers to how the distribution, when calculating a tail, will not remember the given condition ($X > m$) since it doesn't contribute to the end result of a tail even given that condition.

Furthermore, let us perform an interesting mathematical derivation as follows:
\begin{ln-practice}{The Distribution of $\min(X, Y)$}{}
    \textbf{Question}: What is the distribution of $\min(X, Y)$, provided \textit{\textbf{independent}} random variables $X \sim Geometric(p)$ and $Y \sim Geometric(q)$?
    \tcblower
    \textbf{Solution}:
    Here, let us attempt to find the probability at which $\min(X, Y) = k$:
    \begin{align*}
        \P[\min(X, Y) = k]
        &= \P[X = k, Y \geq k] + \P[Y = k, X > k] \\
        &= \P[X = k \cap Y \geq k] + \P[Y = k \cap X > k] \\
        &= {(1 - p)}^{k - 1} p {(1 - q)}^k + {(1 - q)}^{k - 1} q {(1 - p)}^{k - 1} \\
        &= {(1 - p)}^{k - 1}{(1 - q)}^{k - 1} (p(1 - q) + q) = Geometric(1 - (1 - p)(1 - q))
    \end{align*}
    Furthermore, because
    \[X + Y = \min(X, Y) + \max(X, Y)\]
    We can use the distributions of $X \sim Geometric(p)$, $Y \sim Geometric(q)$, and $\min(X, Y) \sim Geometric(1 - (1 - p)(1 - q))$ to infer about $\max(X, Y)$ as a random variable.
\end{ln-practice}

\subsection{Example (Application)}
What other greater application can CS70 be made on other than probabilistic problems, and more specifically, gacha games!? \\
Let us look at the ``Coupon Collector Problem'':
\begin{quote}
    Let us attempt to collect a set of $n$ different cards. \\
    We can get one card by performing one summon, each of the $n$ cards equivalently likely to appear from that summon. \\
    How many summons do we have to do until we have collected at least one copy of every card?
\end{quote}
Let $S_n$ denote the number of summoning we need to perform to collect all $n$ cards, then the distribution of $S_n$ is difficult to compute directly. But, its expected value can be calculated via the gift of linearity. \\
Let $X_i$ be a random variable rerpesenting the number of boxes needed while trying to get the $i^{th}$ new card, starting immediately after the most recent new card's obtain. \\
Then,
\[S_n = X_1 + X_2 + \cdots + X_n\]
Therefore,
\[\E[S_n] = \E[X_1 + X_2 + \cdots + X_n] = \E[X_1] + \E[X_2] + \cdots + \E[X_n]\]

The expected value of $X_1$ would be $1$, since any first card is a new card. \\
Now, how about $X_i$? \\
Each time we perform a summoning, we will get an new card with probability $\frac{n - i}{n}$. We will continue performing summoning, which can be characterized as a coin flip of heads probability $p = \frac{n - i}{n}$ for simplicity. This makes $X_i$ the number of tosses until first head. \\
There, 
\[X_i \sim \text{Geometric}(\frac{n - i}{n})\]
And consequentially,
\[\E[X_i] = \frac{n}{n - i}\]

Coming back to the original simplification of $\E[S_n]$, then:
\[
    \E[S_n] = \sum_{i = 1}^n \frac{n}{n - i} = n\sum_{i = 1}^n \frac{1}{i}
\]
This can be well approximated as
\[
    n\sum_{i = 1}^n \frac{1}{i} \simeq n(\ln{n} + \gamma_E)
\]
Where $\gamma_E=0.5772\dots$ is known as Euler's constant.

\section{Poisson Distribution}
\begin{ln-define}{Poisson Distribution}{}
    A random variable X for which:
    \[\P[X = i] = \frac{\lambda^i}{i!} e^{-\lambda}, i \in \Z^+\]
    is said to have the Poisson distribution with paramerter $\lambda$. \\
    We may mathematically abbreviate such as:
    \[X \sim Poisson(\lambda)\]
\end{ln-define}
This distrbution is used to estimate the number of times an event happens (notated by the random variable $X \sim Poisson(\lambda)$) assuming the average number of occurrence in some periodic unit is $\lambda$, where such parameter may be in any division of measurement unit (say clicks occuring per second, but also clicks occuring per minute). \\
Therefore, while the objective of Poisson is very distinct, it is also a distribution to be carefully utilized along the contexts of a problem. Specifically, depending on the problem's description of length of period (10 seconds? 10 minutes?) on which we model the Poisson distribution, we might need to multiply some given average rate of occurrence with the length of period to obtain the parameter $\lambda$ (once again, stands for average number of occurrence).
Essentially, then, $\lambda$ is the mean of some distribution!

Let us, once again, verify this distribution by testing whether it sums up to $1$:
\begin{align*}
    \sum_{i = 0}^\infty \frac{\lambda^i}{i!} e^{-\lambda}
    &= e^{-\lambda} \sum_{i = 0}^\infty \frac{\lambda^i}{i!} \\
    e^{x} &= 1 + x + \frac{x^2}{2!} + \cdots \\
    e^{-\lambda} \sum_{i = 0}^\infty \frac{\lambda^i}{i!}
    &= e^{-\lambda} e^{\lambda} = 1
\end{align*}

\subsection{Properties of Poisson RV}
For both expectation and variance, we can discover a very elegant identity. \\
\begin{ln-theorem}{Expectation of Poisson RV}{}
    \begin{align*}
        \E[X]
        &= \sum_{i = 0}^\infty i \P[X = i] \\
        &= \sum_{i = 0}^\infty i \frac{\lambda^i}{i!} e^{-\lambda} \\
        &= e^{-\lambda} \sum_{i = 1}^\infty \frac{\lambda^i}{(i - 1)!} \\
        &= e^{-\lambda} \lambda \sum_{i = 1}^\infty \frac{\lambda^{i - 1}}{(i - 1)!} \\
        &= e^{-\lambda} \lambda e^{\lambda} = \lambda
    \end{align*}
\end{ln-theorem}
And as for variance:
\begin{ln-theorem}{Variance of Poisson RV}{}
    \begin{align*}
        \E[X(X - 1)]
        &= \sum_{i = 0}^\infty i(i - 1) \P[X = i] \\
        &= \sum_{i = 0}^\infty i(i - 1) \frac{\lambda^i}{i!} e^{-\lambda} \\
        &= \lambda^2 e^{-\lambda} \sum_{i = 2}^\infty \frac{\lambda^{i - 2}}{(i - 2)!} \\
        &= \lambda^2 e^{-\lambda} e^{\lambda} = \lambda^2 \\
        Var(X) &= \E[X^2] - {(\E[X])}^2 \\
        &= \E[X(X - 1)] + \E[X] - {(\E[X])}^2 \\
        &= \lambda^2 + \lambda - \lambda^2 = \lambda
    \end{align*}
\end{ln-theorem}

\begin{ln-theorem}{Superposition of Poisson Distribution}{}
    \textbf{Theorem}: Let $X \sim Poisson(\lambda)$ and $Y \sim Poisson(\mu)$ \underline{be independent of each other}, then $X + Y \sim Poisson(\lambda + \mu)$.
    \tcblower
    \textbf{Proof}:
    \begin{align*}
        \P[X + Y = k]
        &= \sum_{j = 0}^k \P[X = j, Y = k - j] \\
        &= \sum_{j = 0}^k \P[X = j] \P[Y = k - j] \\
        &= \sum_{j = 0}^k \frac{\lambda^j}{j!} e^{-\lambda} \frac{\mu^{k - j}}{(k - j)!} e^{-\mu} \\
        &= e^{-(\lambda + \mu)} \sum_{j = 0}^k \frac{\lambda^j \mu^{k - j}}{j!(k - j)!} \\
        &= e^{-(\lambda + \mu)} \frac{{(\lambda + \mu)}^k}{k!}
    \end{align*}
\end{ln-theorem}

\subsection{Binomial Distribution and Poisson Distribution}
You may have discovered that binomial distribution and poisson distributions are very similarly used: to estimate the number of times some event occur, provided some metric of occurrence frequency. \\
In fact, this similarity reflects on a mathematical dimension as well:
\begin{ln-theorem}{Poisson as a Limit of Binomial}{}
    \textbf{Theorem}: Let $X \sim Binomial(n, \frac{\lambda}{n})$ where $\lambda > 0$ is constant, then
    \[\forall i \in \N, \lim_{n \rightarrow \infty}\P[X = i] = \frac{\lambda^i}{i!}e^{-\lambda}\]
    \tcblower
    \textbf{Proof}:
    First, let's do some algebraic simplification.
    \begin{align*}
        \P[X = i]
        &= \binom{n}{i} {(\frac{\lambda}{n})}^i {(1 - \frac{\lambda}{n})}^{n - i} \\
        &= \frac{\lambda^i}{i!} \cdot \frac{n!}{(n - i)! n^i} \cdot {(1 - \frac{\lambda}{n})}^{n} \cdot {(1 - \frac{\lambda}{n})}^{-i}
    \end{align*}
    Here,
    \[\frac{n!}{(n - i)! n^i} = \frac{n (n - 1) (n - 2) \cdots (n - i + 1)}{n^i}\]
    This value approaches $1$ as $n$ approaches $\infty$. \\
    Meanwhile, from calculus, we may determine that:
    \[\lim_{n \rightarrow \infty} {(1 - \frac{\lambda}{n})}^n = e^{-\lambda}\]
    And last but not least, since $i$ is a fixed variable,
    \[\lim_{n \rightarrow \infty} {(1 - \frac{\lambda}{n})}^{-i} = 1\]
    Therefore, multiplying these limits together, we obtain that:
    \[\lim_{n \rightarrow \infty} \P[X = i] = \frac{\lambda^i}{i!}e^{-\lambda}\]
\end{ln-theorem}
